{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - py_vollib_vectorized\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - defaults\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install pyarrow yfinance joblib pandas numpy matplotlib seaborn scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bc967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars-lts-cpu\n",
      "  Using cached polars_lts_cpu-1.31.0-cp39-abi3-win_amd64.whl.metadata (15 kB)\n",
      "Using cached polars_lts_cpu-1.31.0-cp39-abi3-win_amd64.whl (35.1 MB)\n",
      "Installing collected packages: polars-lts-cpu\n",
      "Successfully installed polars-lts-cpu-1.31.0\n",
      "Requirement already satisfied: py_vollib_vectorized in c:\\users\\kevin\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: py-vollib>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (1.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (0.61.0)\n",
      "Requirement already satisfied: py-lets-be-rational in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py_vollib_vectorized) (1.15.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from numba>=0.51.0->py_vollib_vectorized) (0.44.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from py-vollib>=1.0.1->py_vollib_vectorized) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas->py_vollib_vectorized) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas->py_vollib_vectorized) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas->py_vollib_vectorized) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->py_vollib_vectorized) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "! pip install polars-lts-cpu\n",
    "! pip install py_vollib_vectorized\n",
    "! pip install git+https://github.com/vollib/py_lets_be_rational.git # overwrite py_vollib_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e35961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the folder with your calibration .py files to the Python path\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\Summer-Quant-2025\\heston calibration\\tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polars version of filtering & enriching SPXW options data\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "import shutil\n",
    "import stat\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Make it a directory, not a “.parquet” file\n",
    "INPUT_PATTERN = r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\spxw_filtered\\*.csv\"\n",
    "OUTPUT_DIR    = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_pre_filtered_parquet\")\n",
    "\n",
    "# Clean up any old output directory\n",
    "def on_rm_error(func, path, exc_info):\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    shutil.rmtree(OUTPUT_DIR, onerror=on_rm_error)\n",
    "\n",
    "# 1. Get min_date, max_date from your options LazyFrame\n",
    "lf = pl.scan_csv(\n",
    "    INPUT_PATTERN,\n",
    "    dtypes={\n",
    "        \"ticker\": pl.Utf8, \"volume\":pl.Int64, \"open\":pl.Float64,\n",
    "        \"close\":pl.Float64, \"high\":pl.Float64, \"low\":pl.Float64,\n",
    "        \"window_start\":pl.Int64, \"transactions\":pl.Int64\n",
    "    }\n",
    ").filter(pl.col(\"window_start\").is_not_null() & (pl.col(\"window_start\") > 0))\n",
    "\n",
    "# Parse ticker and derive date columns for min/max date\n",
    "pat = r\"O:SPXW(\\d{6})([CP])(\\d{8})\"\n",
    "lf_dates = (\n",
    "    lf.with_columns([\n",
    "        pl.col(\"ticker\").str.extract(pat, 1).str.strptime(pl.Date, \"%y%m%d\", strict=False).alias(\"expiration\"),\n",
    "        pl.col(\"window_start\").cast(pl.Datetime(\"ns\")).dt.date().alias(\"date\")\n",
    "    ])\n",
    "    .filter(pl.col(\"expiration\").is_not_null() & pl.col(\"date\").is_not_null())\n",
    ")\n",
    "date_bounds = lf_dates.select([\n",
    "    pl.col(\"date\").min().alias(\"min\"),\n",
    "    pl.col(\"date\").max().alias(\"max\")\n",
    "]).collect()\n",
    "min_date, max_date = date_bounds[\"min\"][0], date_bounds[\"max\"][0]\n",
    "\n",
    "# 2. Build a complete date range as a DataFrame\n",
    "all_dates = pd.DataFrame({\"date\": pd.date_range(min_date, max_date, freq=\"D\").date})\n",
    "\n",
    "# 3. Fetch SPX and IRX, merge with all_dates, and forward-fill\n",
    "spx_pd = (yf.Ticker(\"^GSPC\")\n",
    "           .history(start=min_date, end=max_date + pd.Timedelta(days=1))\n",
    "           [\"Close\"].reset_index().rename(columns={\"Close\":\"S\"}))\n",
    "spx_pd[\"date\"] = spx_pd[\"Date\"].dt.date\n",
    "spx_pd = all_dates.merge(spx_pd[[\"date\", \"S\"]], on=\"date\", how=\"left\").fillna(method=\"ffill\")\n",
    "spx = pl.from_pandas(spx_pd[[\"date\", \"S\"]])\n",
    "\n",
    "irx_pd = (yf.Ticker(\"^IRX\")\n",
    "           .history(start=min_date, end=max_date + pd.Timedelta(days=1))\n",
    "           [\"Close\"].reset_index().rename(columns={\"Close\":\"r\"}))\n",
    "irx_pd[\"date\"] = irx_pd[\"Date\"].dt.date\n",
    "irx_pd[\"r\"] = irx_pd[\"r\"] / 100.0\n",
    "irx_pd = all_dates.merge(irx_pd[[\"date\", \"r\"]], on=\"date\", how=\"left\").fillna(method=\"ffill\")\n",
    "irx = pl.from_pandas(irx_pd[[\"date\", \"r\"]])\n",
    "\n",
    "spx_lazy = spx.lazy()\n",
    "irx_lazy = irx.lazy()\n",
    "\n",
    "# 4. Build the full LazyFrame pipeline\n",
    "# previous steps up to joining S & r\n",
    "lf = (\n",
    "    pl.scan_csv(INPUT_PATTERN, dtypes={ \n",
    "        # specify your dtypes here, e.g.:\n",
    "        \"ticker\": pl.Utf8, \"volume\":pl.Int64, \"open\":pl.Float64,\n",
    "        \"close\":pl.Float64, \"high\":pl.Float64, \"low\":pl.Float64,\n",
    "        \"window_start\":pl.Int64, \"transactions\":pl.Int64\n",
    "    })\n",
    "      .filter(pl.col(\"window_start\") > 0)\n",
    "      .with_columns([\n",
    "         pl.col(\"ticker\").str.extract(pat, 2).replace({\"C\":\"call\",\"P\":\"put\"}).alias(\"option_type\"),\n",
    "         pl.col(\"ticker\").str.extract(pat, 1).str.strptime(pl.Date,\"%y%m%d\",strict=False).alias(\"expiration\"),\n",
    "         pl.col(\"ticker\").str.extract(pat, 3).cast(pl.Int64).alias(\"strike_raw\"),\n",
    "         pl.col(\"window_start\").cast(pl.Datetime(\"ns\")).dt.date().alias(\"date\")\n",
    "      ])\n",
    "      .filter(\n",
    "         pl.col(\"expiration\").is_not_null() &\n",
    "         pl.col(\"strike_raw\").is_not_null() &\n",
    "         pl.col(\"date\").is_not_null()\n",
    "      )\n",
    "      .with_columns([\n",
    "         (pl.col(\"strike_raw\")/1_000).alias(\"strike\"),\n",
    "         ((pl.col(\"expiration\").cast(pl.Datetime(\"ns\")) -\n",
    "           pl.col(\"date\"      ).cast(pl.Datetime(\"ns\")))\n",
    "          .dt.total_days()/365.0).alias(\"T\")\n",
    "      ])\n",
    "      # your T and price filters\n",
    "      .filter((pl.col(\"T\")>7/365)&(pl.col(\"T\")<180/365)&(pl.col(\"close\")>0.1))\n",
    "      # filter out very low volume options\n",
    "      .filter(pl.col(\"volume\") >= 5)\n",
    "      # join in S & r\n",
    "      .join(spx_lazy, on=\"date\", how=\"left\")\n",
    "      .join(irx_lazy, on=\"date\", how=\"left\")\n",
    "\n",
    "      # put–call parity / OTM filter\n",
    "      .with_columns([\n",
    "        # forward price F = S * exp(r * T)\n",
    "        (pl.col(\"S\") * (pl.col(\"r\") * pl.col(\"T\")).exp()).alias(\"F\")\n",
    "      ])\n",
    "      .filter(\n",
    "         ((pl.col(\"option_type\")==\"call\") & (pl.col(\"strike\") >= pl.col(\"F\")))\n",
    "       | ((pl.col(\"option_type\")==\"put\")  & (pl.col(\"strike\") <= pl.col(\"F\")))\n",
    "      )\n",
    "      .drop(\"F\")\n",
    "\n",
    "      # now select your final columns\n",
    "      .select([\"date\",\"strike\",\"T\",\"close\",\"option_type\",\"S\",\"r\"])\n",
    ")\n",
    "\n",
    "# 5. Collect to eager DataFrame\n",
    "df = lf.collect()\n",
    "\n",
    "# 6. Write out partitioned Parquet directory by date\n",
    "df.write_parquet(\n",
    "    OUTPUT_DIR,\n",
    "    compression=\"snappy\",\n",
    "    partition_by=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Pre-filtered & date-partitioned Parquet written to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef271ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid KMeans+PCA+Vega Weighted sampling done!\n",
      "Total picks: 0\n",
      "Sample sizes (first 5): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# KMeans + PCA + Vega Weighted sampling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from py_vollib_vectorized import vectorized_implied_volatility as calculate_iv\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import os\n",
    "import stat\n",
    "\n",
    "def hybrid_kmeans_pca_atm(\n",
    "    df: pd.DataFrame,\n",
    "    n_clusters: int = 200,\n",
    "    n_pca_extremes: int = 0,\n",
    "    atm_width: float = 0.05,\n",
    "    atm_frac: float = 0.20,\n",
    "    num_m_bins: int = 10,             # ← new: how many moneyness bins to enforce\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # 1) days & moneyness\n",
    "    df['days']      = (df['T'] * 365).round().astype(int)\n",
    "    S, r            = df['S'].iloc[0], df['r'].iloc[0]\n",
    "    q               = df.get('q', 0.0)\n",
    "    df['moneyness'] = df['strike'] / S\n",
    "\n",
    "    # 2) implied‐vol + vega\n",
    "    flags = np.where(df['option_type']=='call','c','p')\n",
    "    iv = calculate_iv(\n",
    "        df['close'].values, S, df['strike'].values, df['T'].values,\n",
    "        r, flags, q, model='black_scholes_merton', return_as='numpy'\n",
    "    )/100.0\n",
    "    iv = np.nan_to_num(iv, nan=1e-6)\n",
    "    sqrtT = np.sqrt(df['T'])\n",
    "    d1    = (\n",
    "        np.log(S/df['strike']) +\n",
    "        (r - q + 0.5*iv**2)*df['T']\n",
    "    )/(iv*sqrtT + 1e-12)\n",
    "    df['vega'] = S * np.exp(-q*df['T']) * sqrtT * np.exp(-0.5*d1**2)/np.sqrt(2*np.pi)\n",
    "\n",
    "    # 3) quantile‐transform moneyness & normalize days\n",
    "    max_days     = df['days'].max() or 1\n",
    "    df['m_rank'] = df['moneyness'].rank(pct=True)\n",
    "    df['d_norm'] = df['days'] / max_days\n",
    "\n",
    "    # 4) KMeans → pick max‐vega per cluster\n",
    "    X  = np.vstack([df['m_rank'], df['d_norm']]).T\n",
    "    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    df['cluster'] = km.fit_predict(X)\n",
    "    idx_k        = df.groupby('cluster')['vega'].idxmax().values\n",
    "    samp_k       = df.loc[idx_k]\n",
    "\n",
    "    # 5) PCA extremes (optional)\n",
    "    if n_pca_extremes > 0:\n",
    "        pca     = PCA(n_components=1, random_state=random_state)\n",
    "        df['pc1'] = pca.fit_transform(X).ravel()\n",
    "        half     = n_pca_extremes//2\n",
    "        samp_pc  = pd.concat([\n",
    "            df.nlargest(half, 'pc1'),\n",
    "            df.nsmallest(half, 'pc1')\n",
    "        ])\n",
    "    else:\n",
    "        samp_pc = df.head(0)\n",
    "\n",
    "    # 6) combine & dedupe\n",
    "    combined = pd.concat([samp_k, samp_pc]) \\\n",
    "                 .drop_duplicates(subset=['strike','T','option_type'])\n",
    "\n",
    "    # 7) enforce ATM quota\n",
    "    total_target = n_clusters + n_pca_extremes\n",
    "    atm_target   = int(atm_frac * total_target)\n",
    "    in_atm       = combined[np.abs(combined['moneyness']-1) <= atm_width]\n",
    "    if len(in_atm) < atm_target:\n",
    "        need   = atm_target - len(in_atm)\n",
    "        extras = (df[np.abs(df['moneyness']-1) <= atm_width]\n",
    "                  .drop(combined.index, errors='ignore')\n",
    "                  .nlargest(need, 'vega'))\n",
    "        combined = pd.concat([combined, extras])\n",
    "\n",
    "    # 8) **new**: enforce at least one per m‐bin\n",
    "    # build m‐bin edges in rank‐space\n",
    "    edges = np.linspace(0, 1, num_m_bins+1)\n",
    "    for i in range(num_m_bins):\n",
    "        lo, hi = edges[i], edges[i+1]\n",
    "        # does combined have a pick in [lo,hi)?\n",
    "        mask_comb = combined['moneyness'].rank(pct=True).between(lo, hi, inclusive='left')\n",
    "        if not mask_comb.any():\n",
    "            # pick the highest-vega in df for that slice\n",
    "            mask_df   = df['m_rank'].between(lo, hi, inclusive='left')\n",
    "            if mask_df.any():\n",
    "                pick = df[mask_df].nlargest(1, 'vega')\n",
    "                combined = pd.concat([combined, pick])\n",
    "\n",
    "    # 9) fill to total_target by vega\n",
    "    if len(combined) < total_target:\n",
    "        need      = total_target - len(combined)\n",
    "        leftovers = df.drop(combined.index, errors='ignore')\n",
    "        extra     = leftovers.nlargest(need, 'vega')\n",
    "        combined  = pd.concat([combined, extra])\n",
    "\n",
    "    # 10) cleanup\n",
    "    to_drop = ['days','moneyness','m_rank','d_norm','vega','cluster','pc1']\n",
    "    return combined.drop(columns=[c for c in to_drop if c in combined.columns]) \\\n",
    "                   .reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Parallelize and write per-date\n",
    "# ------------------------------------------------------------------------------\n",
    "PRE_FILTERED_DIR = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_pre_filtered_parquet\")\n",
    "OUTPUT_DIR       = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_sampled_hybrid_parquet\")\n",
    "\n",
    "# Cleanup existing output\n",
    "def _on_rm_error(func, path, exc_info):\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    func(path)\n",
    "\n",
    "if OUTPUT_DIR.exists():\n",
    "    shutil.rmtree(OUTPUT_DIR, onerror=_on_rm_error)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_date(part_dir: Path):\n",
    "    df = pl.read_parquet(str(part_dir/\"0.parquet\")).to_pandas()\n",
    "    sampled = hybrid_kmeans_pca_atm(df)\n",
    "    sampled['date'] = part_dir.name.split('=')[1]\n",
    "    out_dir = OUTPUT_DIR / part_dir.name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pl.from_pandas(sampled).write_parquet(str(out_dir/\"0.parquet\"), compression='snappy')\n",
    "    return len(sampled)\n",
    "\n",
    "# Run in parallel\n",
    "date_dirs = sorted(PRE_FILTERED_DIR.glob(\"date=*\"))\n",
    "sizes = Parallel(n_jobs=-1, verbose=5)(delayed(process_date)(d) for d in date_dirs)\n",
    "\n",
    "print(\"Hybrid KMeans+PCA+Vega Weighted sampling done!\")\n",
    "print(\"Total picks:\", sum(sizes))\n",
    "print(\"Sample sizes (first 5):\", sizes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sampled options data and visualize against full options universe\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLED_PARQUET_DIR = r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_sampled_hybrid_parquet\"\n",
    "PRE_FILTERED_PARQUET_DIR = r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_pre_filtered_parquet\"\n",
    "\n",
    "# Gather all available pre-filtered parquet files (look for '0.parquet')\n",
    "pre_filtered_files = []\n",
    "for root, dirs, files in os.walk(PRE_FILTERED_PARQUET_DIR):\n",
    "    for file in files:\n",
    "        if file == '0.parquet':\n",
    "            pre_filtered_files.append(os.path.join(root, file))\n",
    "\n",
    "if not pre_filtered_files:\n",
    "    raise FileNotFoundError(f\"No '0.parquet' files found in {PRE_FILTERED_PARQUET_DIR}\")\n",
    "\n",
    "# Load all pre-filtered data (full options universe)\n",
    "df_all = pl.concat([pl.read_parquet(f) for f in pre_filtered_files]).to_pandas()\n",
    "\n",
    "# List all date partitions\n",
    "partitions = [d for d in os.listdir(SAMPLED_PARQUET_DIR) if os.path.isdir(os.path.join(SAMPLED_PARQUET_DIR, d))]\n",
    "partitions.sort()\n",
    "sampled_partitions = random.sample(partitions, min(10, len(partitions)))\n",
    "\n",
    "print(\"Sampled partitions:\", sampled_partitions)\n",
    "\n",
    "for part in sampled_partitions:\n",
    "    # Extract date from partition name (assumes format date=YYYY-MM-DD)\n",
    "    date_str = part.split('=')[1] if '=' in part else part\n",
    "    part_dir = os.path.join(SAMPLED_PARQUET_DIR, part)\n",
    "    parquet_files = [os.path.join(part_dir, f) for f in os.listdir(part_dir) if f == '0.parquet']\n",
    "    if not parquet_files:\n",
    "        print(f\"No '0.parquet' file found in {part_dir}\")\n",
    "        continue\n",
    "    df_sampled = pl.read_parquet(parquet_files).to_pandas()\n",
    "    df_full = df_all[df_all['date'].astype(str) == date_str]\n",
    "    print(f\"\\nPartition: {part}\")\n",
    "    print(\"Sampled shape:\", df_sampled.shape, \"Full shape:\", df_full.shape)\n",
    "    print(df_sampled.head(5))\n",
    "    # Visualization: overlay sampled and full data\n",
    "    if all(col in df_sampled.columns for col in ['strike', 'S', 'T']) and all(col in df_full.columns for col in ['strike', 'S', 'T']):\n",
    "        moneyness_full = df_full['strike'] / df_full['S']\n",
    "        expiry_full = df_full['T'] * 365\n",
    "        moneyness_sampled = df_sampled['strike'] / df_sampled['S']\n",
    "        expiry_sampled = df_sampled['T'] * 365\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.scatter(moneyness_full, expiry_full, s=10, c='gray', alpha=0.3, label='All Options')\n",
    "        plt.scatter(moneyness_sampled, expiry_sampled, s=60, c='tab:orange', edgecolor='black', marker='o', label='Sampled')\n",
    "        plt.xlabel('Moneyness (K/S)')\n",
    "        plt.ylabel('Days to Expiry')\n",
    "        plt.title(f\"Sampled vs All Options: {date_str}\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Required columns for visualization not found in this partition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb504ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust pre-check for sampled options data\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "SAMPLED_DIR = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_sampled_hybrid_parquet\")\n",
    "\n",
    "# 1) Load all sampled partitions into a single pandas DataFrame\n",
    "df_sampled_all = (\n",
    "    pl.scan_parquet(f\"{SAMPLED_DIR}/date=*/*.parquet\")\n",
    "      .collect()\n",
    "      .to_pandas()\n",
    ")\n",
    "\n",
    "# 2) Sanity‐check required columns\n",
    "required_cols = ['date', 'strike', 'T', 'close', 'option_type', 'S', 'r']\n",
    "missing = [c for c in required_cols if c not in df_sampled_all.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required cols in sampled data: {missing}\")\n",
    "\n",
    "# 3) Basic emptiness & date‐count checks\n",
    "if df_sampled_all.empty:\n",
    "    raise ValueError(\"No sampled data found!\")\n",
    "unique_dates = df_sampled_all['date'].unique()\n",
    "print(f\"[Pre-Check] Dates covered: {len(unique_dates)}  (e.g. {unique_dates[:3]})\")\n",
    "\n",
    "# 4) Inspect the very first date's slice\n",
    "first_date = unique_dates[0]\n",
    "df0 = df_sampled_all[df_sampled_all['date'] == first_date]\n",
    "print(f\"\\n[Pre-Check] Date = {first_date} → {len(df0)} rows\")\n",
    "\n",
    "for col in required_cols[1:]:  # skip 'date'\n",
    "    ser     = df0[col]\n",
    "    dtype   = getattr(ser, 'dtype', type(ser.iloc[0]))\n",
    "    n_nan   = ser.isna().sum()\n",
    "    print(f\"  • {col:12s} dtype={dtype},  rows={ser.shape[0]},  NaNs={n_nan}\")\n",
    "\n",
    "# 5) Convert to numpy and check\n",
    "strikes = np.asarray(df0['strike'], dtype=float)\n",
    "T       = np.asarray(df0['T'], dtype=float)\n",
    "prices  = np.asarray(df0['close'], dtype=float)\n",
    "flags   = np.where(df0['option_type']=='call','c','p')\n",
    "r_arr   = np.full_like(strikes, df0['r'].iloc[0], dtype=float)\n",
    "q_arr   = np.zeros_like(strikes, dtype=float)\n",
    "\n",
    "print(\"\\n[Pre-Check] NumPy arrays:\")\n",
    "for name, arr in [\n",
    "    ('strikes', strikes), ('T', T), ('prices', prices),\n",
    "    ('flags', flags), ('r_arr', r_arr), ('q_arr', q_arr)\n",
    "]:\n",
    "    arr = np.asarray(arr)\n",
    "    print(f\"  • {name:8s} shape={arr.shape}, dtype={arr.dtype}\", end='')\n",
    "    if np.issubdtype(arr.dtype, np.number):\n",
    "        print(f\", NaNs={np.isnan(arr).sum()}\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "# 6) Warn on degenerate arrays\n",
    "for name, arr in [('strikes', strikes), ('T', T), ('prices', prices)]:\n",
    "    if arr.size == 0:\n",
    "        print(f\"  [Warning] {name} is empty!\")\n",
    "    elif np.all(np.isnan(arr)):\n",
    "        print(f\"  [Warning] {name} all NaN!\")\n",
    "    elif np.all(arr == 0):\n",
    "        print(f\"  [Warning] {name} all zero!\")\n",
    "\n",
    "print(\"\\n[Pre-Check] Completed successfully – ready for calibration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da498ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration over a date range with full worker logic, now with LM progress logs returned\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from Heston_COS_METHOD import heston_cosine_method\n",
    "from Levenberg_Marquardt import levenberg_Marquardt\n",
    "from Heston_Calibration_Class import Data_Class\n",
    "from py_vollib_vectorized import vectorized_implied_volatility as calculate_iv\n",
    "\n",
    "# === CONFIG ===\n",
    "SAMPLED_DIR = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_sampled_hybrid_parquet\")\n",
    "CALIB_DIR   = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_calibrated_parquet\")\n",
    "CALIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === SELECT DATE WINDOW ===\n",
    "start_date = datetime(2024, 1, 2).date()\n",
    "end_date   = datetime(2024, 2, 1).date()\n",
    "\n",
    "def calibrate_one_date(part_dir: Path):\n",
    "    date = part_dir.name.split('=')[1]\n",
    "    print(f\"→ Starting calibration for {date}\")\n",
    "\n",
    "    # Read that date's Parquet(s)\n",
    "    dfs = [pl.read_parquet(f) for f in part_dir.glob(\"*.parquet\")]\n",
    "    df_pl = pl.concat(dfs)\n",
    "\n",
    "    # Extract arrays\n",
    "    S       = float(df_pl[\"S\"][0])\n",
    "    strikes = df_pl[\"strike\"].to_numpy()\n",
    "    T       = df_pl[\"T\"].to_numpy()\n",
    "    prices  = df_pl[\"close\"].to_numpy()\n",
    "    flags   = np.where(df_pl[\"option_type\"].to_numpy() == \"call\", \"c\", \"p\")\n",
    "    r       = float(df_pl[\"r\"][0])\n",
    "    q       = 0.0\n",
    "    r_arr   = np.full_like(strikes, r, dtype=float)\n",
    "    q_arr   = np.full_like(strikes, q, dtype=float)\n",
    "\n",
    "    # Build calibration container\n",
    "    data = Data_Class()\n",
    "    data.S             = S\n",
    "    data.K             = strikes\n",
    "    data.T             = T\n",
    "    data.r             = r_arr\n",
    "    data.q             = q_arr\n",
    "    data.market_prices = prices\n",
    "    data.flag          = flags\n",
    "    data.calculate_implied_vol()\n",
    "    market_vol = data.market_vol\n",
    "\n",
    "    # Calibration parameters\n",
    "    initial_guess       = np.array([0.04, 0.50, -0.70, 1.0, 0.04]).reshape(5,1)\n",
    "    N, L                = 100, 10\n",
    "    I, w                = 500, 1e-3\n",
    "    precision           = 0.01\n",
    "    params_to_calibrate = ['vbar','sigma','rho','kappa','v0']\n",
    "    accel_mag, min_acc  = 1, 1e-3\n",
    "\n",
    "    # Run Levenberg–Marquardt and collect logs\n",
    "    calib_params, acc, rej, RMSE, lm_logs = levenberg_Marquardt(\n",
    "        data, initial_guess, I, w, N, L, precision,\n",
    "        params_to_calibrate, accel_mag, min_acc, return_logs=True\n",
    "    )\n",
    "\n",
    "    # Price via Heston COS\n",
    "    h_prices = heston_cosine_method(\n",
    "        data.S, strikes, T, N, L, r_arr, q_arr,\n",
    "        calib_params[0,0], calib_params[4,0],\n",
    "        calib_params[1,0], calib_params[2,0],\n",
    "        calib_params[3,0], flags\n",
    "    )\n",
    "\n",
    "    # Implied vol of Heston prices\n",
    "    calib_iv = (\n",
    "        calculate_iv(\n",
    "            h_prices, S, strikes, T, r_arr, flags, q_arr,\n",
    "            model='black_scholes_merton', return_as='numpy'\n",
    "        ) * 100\n",
    "    ).ravel()\n",
    "\n",
    "    # Assemble output DataFrame\n",
    "    out = pl.DataFrame({\n",
    "        \"date\": [date]*len(strikes),\n",
    "        \"strike\": strikes,\n",
    "        \"T\": T,\n",
    "        \"close\": prices,\n",
    "        \"option_type\": df_pl[\"option_type\"].to_numpy(),\n",
    "        \"S\": [S]*len(strikes),\n",
    "        \"r\": [r]*len(strikes),\n",
    "        \"q\": [q]*len(strikes),\n",
    "        \"market_vol\": market_vol,\n",
    "        \"calib_iv\": calib_iv,\n",
    "        \"calib_vbar\": float(calib_params[0,0]),\n",
    "        \"calib_sigma\": float(calib_params[1,0]),\n",
    "        \"calib_rho\": float(calib_params[2,0]),\n",
    "        \"calib_kappa\": float(calib_params[3,0]),\n",
    "        \"calib_v0\": float(calib_params[4,0]),\n",
    "        \"calib_rmse\": float(RMSE[-1]) if len(RMSE)>0 else np.nan,\n",
    "        \"calib_acc\": int(acc),\n",
    "        \"calib_rej\": int(rej),\n",
    "    })\n",
    "\n",
    "    # Write partitioned by date\n",
    "    out_dir = CALIB_DIR / f\"date={date}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out.write_parquet(out_dir/\"part-0.parquet\", compression=\"snappy\")\n",
    "\n",
    "    print(f\"✓ Finished calibration for {date}\")\n",
    "    return date, lm_logs\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "all_date_dirs = sorted(SAMPLED_DIR.glob(\"date=*\"))\n",
    "# Filter to your desired window\n",
    "date_dirs = []\n",
    "for d in all_date_dirs:\n",
    "    dstr = d.name.split('=')[1]\n",
    "    ddate = datetime.strptime(dstr, \"%Y-%m-%d\").date()\n",
    "    if start_date <= ddate <= end_date:\n",
    "        date_dirs.append(d)\n",
    "\n",
    "print(f\"Running calibration on {len(date_dirs)} days from {start_date} to {end_date}\")\n",
    "\n",
    "start = time.time()\n",
    "processed = Parallel(n_jobs=-1, verbose=5)(\n",
    "    delayed(calibrate_one_date)(d) for d in date_dirs\n",
    ")\n",
    "total_min = (time.time() - start) / 60\n",
    "print(f\"\\n✅ Calibrated {len(processed)} dates in {total_min:.1f} minutes\")\n",
    "\n",
    "# Print LM logs for each date\n",
    "for date, lm_logs in processed:\n",
    "    print(f\"\\n--- LM Progress for {date} ---\")\n",
    "    for line in lm_logs:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   4 out of  22 | elapsed:    5.7s remaining:   26.1s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of  22 | elapsed:    5.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=12)]: Done   4 out of  22 | elapsed:    5.7s remaining:   26.1s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of  22 | elapsed:    5.9s remaining:    8.6s\n",
      "[Parallel(n_jobs=12)]: Done  14 out of  22 | elapsed:    8.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=12)]: Done  19 out of  22 | elapsed:    8.7s remaining:    1.3s\n",
      "[Parallel(n_jobs=12)]: Done  14 out of  22 | elapsed:    8.6s remaining:    4.9s\n",
      "[Parallel(n_jobs=12)]: Done  19 out of  22 | elapsed:    8.7s remaining:    1.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Surface] done: 22/22 succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  22 out of  22 | elapsed:    9.3s finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from Heston_Calibration_Class import Data_Class\n",
    "\n",
    "def plot_surface_for_date(date: str, calib_dir: Path, out_dir: Path):\n",
    "    day_dir = calib_dir / f\"date={date}\"\n",
    "    files = list(day_dir.glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        print(f\"[Surface] {date} → no data, skipping\")\n",
    "        return False\n",
    "\n",
    "    # load into pandas for Data_Class\n",
    "    df = pl.concat([pl.read_parquet(p) for p in files]).to_pandas()\n",
    "    if df.empty:\n",
    "        print(f\"[Surface] {date} → empty DataFrame, skipping\")\n",
    "        return False\n",
    "\n",
    "    # build Data_Class\n",
    "    data = Data_Class()\n",
    "    data.S             = float(df[\"S\"].iloc[0])\n",
    "    data.K             = df[\"strike\"].values.astype(float)\n",
    "    data.T             = df[\"T\"].values.astype(float)\n",
    "    data.r             = np.full_like(data.K, float(df[\"r\"].iloc[0]), dtype=float)\n",
    "    data.q             = np.zeros_like(data.K, dtype=float)\n",
    "    data.market_prices = df[\"close\"].values.astype(float)\n",
    "    data.flag          = np.where(df[\"option_type\"]==\"call\",\"c\",\"p\")\n",
    "    data.calculate_implied_vol()\n",
    "\n",
    "    calib_iv = df[\"calib_iv\"].values.astype(float)\n",
    "\n",
    "    # ensure output folder exists\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # switch cwd so plot_save_surface writes <date>.png into out_dir\n",
    "    prev_cwd = os.getcwd()\n",
    "    os.chdir(str(out_dir))\n",
    "    try:\n",
    "        data.plot_save_surface(calib_iv, date_today=date)\n",
    "        print(f\"[Surface] {date} → saved {date}.png in {out_dir}\")\n",
    "    finally:\n",
    "        os.chdir(prev_cwd)\n",
    "\n",
    "    return True\n",
    "\n",
    "# ------------- usage -------------\n",
    "from joblib import Parallel\n",
    "import os\n",
    "\n",
    "CALIB_PARQUET_DIR = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_calibrated_parquet\")\n",
    "SURFACE_PNG_DIR   = Path(r\"C:\\Users\\kevin\\OneDrive\\Documents\\Draco\\data\\spxw_surface_png\")\n",
    "\n",
    "all_dates = sorted(d.name.split(\"=\")[1]\n",
    "                   for d in CALIB_PARQUET_DIR.glob(\"date=*\") if d.is_dir())\n",
    "# pick your window, or use all_dates:\n",
    "sel = [d for d in all_dates if \"2024-01-02\" <= d <= \"2024-02-01\"]\n",
    "\n",
    "results = Parallel(n_jobs=os.cpu_count(), verbose=5)(\n",
    "    delayed(plot_surface_for_date)(date, CALIB_PARQUET_DIR, SURFACE_PNG_DIR)\n",
    "    for date in sel\n",
    ")\n",
    "\n",
    "print(f\"[Surface] done: {sum(results)}/{len(sel)} succeeded\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
